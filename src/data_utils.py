import numpy as np

import torch
from transformers import AutoTokenizer


class PreprocessDataset(torch.utils.data.Dataset):

    def __init__(self, sentences, targets, tokenizer, max_len):
        """
        Custom PyTorch dataset that preprocess each sentence.
	
        Attributes:
            sentences (numpy.array) representing raw text input
            targets (numpy.array) representing encoded target labels
            tokenizer (transformers.AutoTokenizer) representing pre-trained tokenizer
            max_len (int) representing the max length to pad the sequences accordingly
                
        """
        super(PreprocessDataset, self).__init__()
        self.sentences = sentences
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, item):
        inputs = [str(self.sentences[item])]
        target = self.targets[item]

        encoding = [
            self.tokenizer.encode_plus(
                x,
                add_special_tokens=True,
                max_length=self.max_len,
                return_token_type_ids=True,
                padding="max_length",
                return_attention_mask=True,
                return_tensors="pt",
                truncation=True
            ) for x in inputs
        ]

        # concatenating entries generated by tokenizer
        input_ids = torch.cat(tuple(x["input_ids"].flatten() for x in encoding), dim=0)
        attention_mask = torch.cat(tuple(x["attention_mask"].flatten() for x in encoding), dim=0)

        return {
            "input_text": inputs,
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "targets": torch.tensor(target, dtype=torch.long)
        }


def create_data_loader(inputs: np.array,
                       targets: np.array,
                       tokenizer: AutoTokenizer,
                       max_len: int,
                       batch_size: int) -> torch.utils.data.DataLoader:
    """
    Function that creates a data loader from the custom dataset, in order to feed it to the neural network.
		
		Args: 
            inputs (numpy.array): raw text input
            targets (numpy.array): encoded target labels
            tokenizer (transformers.AutoTokenizer): pre-trained tokenizer
            max_len (int): max length to pad the sequences accordingly
            batch_size (int): batch size to feed the neural network in each training step

		Returns: 
			PyTorch DataLoader of tokenized sequences, mask inputs and the associated target labels
    """
    ds = PreprocessDataset(inputs, targets, tokenizer, max_len)
    return torch.utils.data.DataLoader(ds, batch_size=batch_size, num_workers=0, drop_last=True)



    