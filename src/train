#!/usr/bin/env python3

import os
import sys
import json
from collections import defaultdict
import logging

import pandas as pd
import numpy as np
import nltk

from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import  AutoTokenizer, AutoModel, get_linear_schedule_with_warmup

from data_utils import create_data_loader
from model import BERTClassifier, save_model_state, EMBEDDING_MODEL_PATH, SEED
from evaluation_utils import get_predictions, evaluate_classification


logging.basicConfig(level=logging.DEBUG, format="[%(asctime)s] %(levelname)s - %(message)s")
logger = logging.getLogger()


PREFIX = "/opt/ml/"
INPUT_PATH = os.path.join(PREFIX, "input/data")
_DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

np.random.seed(SEED)
torch.manual_seed(SEED)


def get_arguments() -> dict:
    """"
    Function that reads the hyperparameters from SageMaker training job
    """
    with open(os.path.join(PREFIX, "input/config/hyperparameters.json"), "r") as tc:
        trainingParams = json.load(tc)
    return trainingParams


def load_data(file_path, channel):
    """"
    Function that loads the data from each channel in S3 buckets

        Args: 
            file_path (str): path to S3 bucket
            channel (str): `train` or `test`

        Returns: 
           Pandas DataFrame containing data (target labels and input texts)
    """
    
    input_files = [ os.path.join(file_path, file) for file in os.listdir(file_path) ]
    raw_data = [ pd.read_csv(file) for file in input_files ]
    df = pd.concat(raw_data)
    return df


def epoch_step(model: nn.Module, data_loader: torch.utils.data.DataLoader, loss_function: nn.Module,
               optimizer: optim.Optimizer, scheduler: optim.Optimizer, current_epoch: int,
               print_every: int = 10, clip: bool = True, backprop: bool = True, verbose: bool = False) -> float:
    """"
    Function that performs one epoch step either in the training or the validation set

        Args: 
            model (nn.Module): deep learning model
            data_loader (torch.utils.data.DataLoader): Tokenized sequences, mask inputs and the associated target labels
            loss_function (nn.Module): Loss function to evluate performance and compute gradients
            optimizer (optim.Optimizer): Optimizer that will update weights
            scheduler (optim.Optimizer): Scheduler to slowly reduces learning rate across epochs
            current_epoch (int): Number of current epoch
            print_every (int): Number of steps after which status is printed
            clip (bool): If True, gradient norms are clipped
            backprop (bool): If True, training stage with backpropagation
            verbose (bool): If True, information is displayed on screen

        Returns: 
           Computed mean epoch loss
    """

    losses = []
    for batch_idx, batch_data in enumerate(data_loader):
        targets = batch_data["targets"].to(_DEVICE)

        outputs = model(
            input_ids=batch_data["input_ids"].to(_DEVICE),
            attention_mask=batch_data["attention_mask"].to(_DEVICE)
        )

        loss = loss_function(outputs, targets.unsqueeze(1).float())
        losses.append(loss.item())

        if verbose:
            if (batch_idx+1) % print_every == 0:
                text = f"\tEpoch: {current_epoch} - Batch: {batch_idx+1}/{len(data_loader)} - Step Loss: {losses[-1]:.4f}"
                logger.info(text)

        if backprop:
            loss.backward()
        
            if clip:
                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            if scheduler is not None:
                scheduler.step()

            optimizer.zero_grad()

    return np.mean(losses)


def train(model: nn.Module, train_data_loader: torch.utils.data.DataLoader, val_data_loader: torch.utils.data.DataLoader,
          epochs: int, optimizer: optim.Optimizer, scheduler: optim.Optimizer, loss_function: nn.Module,
          checkpoint: bool = False, checkpoint_path: str = None, max_epochs_without_improvement: int = 5, 
          verbose: bool = False):
    """"
    Function that trains the model

        Args: 
            model (nn.Module): deep learning model
            train_data_loader (torch.utils.data.DataLoader): Training set containing okenized sequences, mask inputs and the associated target labels
            val_data_loader (torch.utils.data.DataLoader): Validation set containing okenized sequences, mask inputs and the associated target labels
            epochs (int): Total number of epochs to perform training
            optimizer (optim.Optimizer): Optimizer that will update weights
            scheduler (optim.Optimizer): Scheduler to slowly reduces learning rate across epochs
            loss_function (nn.Module): Loss function to evluate performance and compute gradients
            checkpoint (bool): If True, current model is saved once it improves current performance
            checkpoint_path (str): Path where checkpoint models should be saved
            max_epochs_without_improvement (int): Number of epochs to apply early stopping
            verbose (bool): If True, information is displayed on screen

        Returns: 
           Computed mean epoch loss
    """

    history = defaultdict(list)
    model = model.to(_DEVICE)
    best_loss = np.inf
    early_stopping_count = 0

    for epoch in range(epochs):
        logger.info(f"Starting Epoch {epoch+1}/{epochs}...")
        model = model.train()
        train_loss = epoch_step(
                model=model,
                data_loader=train_data_loader,
                loss_function=loss_function,
                optimizer=optimizer,
                scheduler=scheduler,
                current_epoch=epoch+1,
                print_every=10,
                clip=True,
                backprop=True,
                verbose=verbose
        )

        model = model.eval()
        with torch.no_grad():
            val_loss = epoch_step(
                model=model,
                data_loader=val_data_loader,
                loss_function=loss_function,
                optimizer=None,
                scheduler=None,
                current_epoch=epoch+1,
                print_every=10,
                clip=False,
                backprop=False,
                verbose=False
        )

        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)

        logger.info(f"Epoch: {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        if val_loss >= best_loss:
            early_stopping_count += 1
            logger.info(f"Strike: {early_stopping_count}/{max_epochs_without_improvement}")
        else:
            early_stopping_count = 0
            best_loss = val_loss
            if checkpoint:
                logger.info("Dumping model checkpoint...")
                model.to(torch.device("cpu"))
                torch.save(save_model_state(model),
                           os.path.join(checkpoint_path, f"checkpoint-val_loss-{val_loss}.pth"))
                model = model.to(_DEVICE)

        if early_stopping_count == max_epochs_without_improvement:
            logger.warning("Training end triggered by early stopping!!")
            break

    return history


if __name__ == "__main__":
    logger.info("TRAINING STARTED...")
    try:

        trainingParams = get_arguments()

        n_epochs = int(trainingParams.get('epochs', 50))
        batch_size = int(trainingParams.get('batch-size', 10))
        learning_rate = float(trainingParams.get('learning-rate', 3e-5))
        max_sentence_length_choice = trainingParams.get('max-sentence-length-choice', "mean")
        n_classes = int(trainingParams.get('n-classes', 1))
        n_hidden = int(trainingParams.get('n-hidden', 16))
        n_layers = int(trainingParams.get('n-layers', 1))
        dropout_ratio = float(trainingParams.get('dropout-ratio', 0.2))
        embeddings_grad = not (int(trainingParams.get('embeddings-grad', 0)) == 0)
        max_epochs_without_improvement = int(trainingParams.get('max-epochs-without-improvement', 3))
        class_weights = not (int(trainingParams.get('class-weights', 0)) == 0)
        threshold = float(trainingParams.get('threshold', 0.5))
        verbose = not (int(trainingParams.get('verbose', 0)) == 0)
        quantize = not (int(trainingParams.get('quantize', 0)) == 0)
        checkpoint = not (int(trainingParams.get('checkpoint', 0)) == 0)
        output_data_dir = trainingParams.get('output-data-dir', os.path.join(PREFIX, "output"))
        model_dir = trainingParams.get('model-dir', os.path.join(PREFIX, "model"))
        train_path = trainingParams.get('train', os.path.join(INPUT_PATH, "train"))
        test_path = trainingParams.get('test', os.path.join(INPUT_PATH, "test"))
        checkpoint_path = trainingParams.get('checkpoint', os.path.join(PREFIX, "checkpoints"))
        hidden_size = [n_hidden] * n_layers

        logger.info("PARAMETERS LOADED!")

        train_data = load_data(train_path, "train")
        test_data = load_data(test_path, "test")

        logger.info("DATA LOADED!")

        if isinstance(max_sentence_length_choice, str):
            max_sentence_length = pd.concat([
                train_data.input.apply(lambda x: len(nltk.word_tokenize(x))),
                test_data.input.apply(lambda x: len(nltk.word_tokenize(x))),
            ], axis=0)

            if max_sentence_length_choice == "max":
                max_sentence_length = int(max_sentence_length.max())
            elif max_sentence_length_choice == "mean":
                max_sentence_length = int(max_sentence_length.mean() + max_sentence_length.std())
            elif max_sentence_length_choice == "most":
                max_sentence_length = int(max_sentence_length.describe()["75%"] + max_sentence_length.std())
            else:
                raise ValueError("`max_sentence_length_choice` must be an integer or one of the following [mean, most, max]")
        elif isinstance(max_sentence_length_choice, int):
            max_sentence_length = max_sentence_length_choice
        else:
            raise ValueError("`max_sentence_length_choice` must be an integer or one of the following [mean, most, max]")

        train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=SEED, stratify=train_data.target)

        tokenizer_model = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_PATH)
        embedding_model = AutoModel.from_pretrained(EMBEDDING_MODEL_PATH)

        logger.info("PRE-TRAINED EMBEDDINGS AND TOKENIZER LOADED!")
        
        train_data_loader = create_data_loader(
            inputs=train_data.input.values,
            targets=train_data.target.values,
            tokenizer=tokenizer_model,
            max_len=max_sentence_length,
            batch_size=batch_size
        )
        val_data_loader = create_data_loader(
            inputs=val_data.input.values,
            targets=val_data.target.values,
            tokenizer=tokenizer_model,
            max_len=max_sentence_length,
            batch_size=batch_size
        )

        logger.info("DATA LOADERS LOADED!")

        model = BERTClassifier(n_classes=n_classes, embedding_model=embedding_model, 
                               max_length=max_sentence_length, hidden_size=hidden_size,
                               dropout_ratio=dropout_ratio, embeddings_grad=embeddings_grad)
        
        logger.info("MODEL CREATED!")

        if class_weights:
            if n_classes == 1:
                class_weights = round(train_data.target.mean(), 2)
                loss_function = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([class_weights])).to(_DEVICE)
            else:
                class_weights = np.round( train_data.value_counts().sort_index().values / len(train_data), 2)
                loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor(list(class_weights))).to(_DEVICE)
        else:
            if n_classes == 1:
                loss_function = nn.BCELoss().to(_DEVICE)
            else:
                loss_function = nn.CrossEntropyLoss().to(_DEVICE)

        logger.info("LOSS FUNCTION SET UP!")

        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=0,
            num_training_steps=len(train_data_loader) * n_epochs
        )

        logger.info("OPTIMIZER AND SCHEDULER SET UP!")

        logger.info("TRAINING NN...")
        history = train(
            model=model,
            train_data_loader=train_data_loader,
            val_data_loader=val_data_loader,
            epochs=n_epochs,
            optimizer=optimizer,
            scheduler=scheduler,
            loss_function=loss_function,
            checkpoint=checkpoint,
            checkpoint_path=checkpoint_path,
            max_epochs_without_improvement=max_epochs_without_improvement,
            verbose=verbose
        )
        logger.info("TRAINING COMPLETED!!")

        logger.info("PREDICTING THE TEST DATA...")
        model.to(torch.device("cpu"))
        model.eval()

        if quantize:
            model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)

        y_pred = []
        for x in zip(test_data.input.to_list()):
            y_pred.append(get_predictions(model, tokenizer_model, x[0], dynamic_input=True, gpu=False))
        y_proba = np.array(y_pred)

        if n_classes == 1:
            y_pred = np.int8(y_proba > threshold)
        else:
            y_pred = np.argmax(y_proba, axis=1)
        logger.info("PREDICTIONS COMPLETED!")

        logger.info("EVALUATING PERFORMANCE...")
        metrics = evaluate_classification(test_data.target.tolist(), y_pred, y_proba)
        for k, v in metrics.items():
            logger.info(f"{k} = {v:.4f}")

        logger.info("SAVING MODEL...")
        torch.save(save_model_state(model), os.path.join(model_dir, "model.pth"))
        with open(os.path.join(output_data_dir, "metrics.json"), "w") as output_file:
            json.dump(metrics, output_file)
        logger.info("MODEL SAVED!")

    except Exception as e:
        logger.error(f"Exception during training:\n{e}")
        sys.exit(255)

    sys.exit(0)